{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1kfRxITD66kM_hllWfsdrq3gFx9VLHs7A","timestamp":1700220671053}],"authorship_tag":"ABX9TyOFRVgln2F2u6C+Y6FAYWYX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LwLvufqPCjLy"},"outputs":[],"source":["!pip install datasets\n","!pip install transformers\n","\n","from datasets import load_dataset\n","from transformers import BertTokenizer\n","from transformers import TFBertForSequenceClassification\n","import tensorflow as tf\n","\n","# NSMC 데이터셋을 가져오기\n","dataset = load_dataset(\"nsmc\")\n","\n","# train_data와 test_data를 추출하기\n","train_data = dataset[\"train\"]\n","test_data = dataset[\"test\"]\n","\n","# 텍스트와 레이블 추출\n","train_texts = train_data[\"document\"]\n","train_labels = train_data[\"label\"]\n","test_texts = test_data[\"document\"]\n","test_labels = test_data[\"label\"]"]},{"cell_type":"code","source":["# Bert 토크나이저를 가져오기\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","# train 과 test 데이터셋을 토큰화\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True)"],"metadata":{"id":"eXXit6IaCxXK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터를 텐서플로우 데이터셋으로 변환\n","train_dataset = tf.data.Dataset.from_tensor_slices((\n","    dict(train_encodings),\n","    train_labels\n","))\n","\n","test_dataset = tf.data.Dataset.from_tensor_slices((\n","    dict(test_encodings),\n","    test_labels\n","))"],"metadata":{"id":"CTPmoFSBC2yz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bert 모델 가져오기\n","model = TFBertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","# 모델 컴파일\n","model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","# 모델 학습\n","model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=64, validation_data=test_dataset.batch(16))"],"metadata":{"id":"rCa53aIrC5ES"},"execution_count":null,"outputs":[]}]}